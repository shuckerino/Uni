1. In der Vorlesung wurde ein Bewertungsmaß auf Folie 10 für die Wumpus-Welt vorgestellt. Wie werden diese Werte ermittelt? Kann man die Werte versuchen zu optimieren, sodass mein Agent spezifische Sachen besser machen kann, bzw. kann man die Bewertungsscores an die Wahrscheinlichkeit der Ereignisse in der Welt des Agenten anpassen?

2. Wenn wir das Beispiel der Wumpus-Welt ansehen, dann ist diese ziemlich klein. Dennoch müssen wir für diese schon viele Regeln und Aktionen beachten. In der realen Welt gibt es so viele mögliche Einflussfaktoren, wodurch mir die Frage kommt, bis wann man tatsächlich symbolische KI noch einsetzen kann. Da sich die letzten Jahre mehr das maschinelle Lernen durchgesetzt hat, da dieses auch für komplett neue Inputs möglichst gute Ausgaben erzeugen kann. Dies ist bei symbolische KI ja anders, wenn ich einen Zustand entdecke, für den ich keine Regeln definiert habe, dann kann ich nicht dementsprechend handeln. Nun zu meiner Frage: gerade für immer komplexer werdende Systeme (nehmen wir humanoide Roboter als Hilfe im Haus), inwiefern spielt symbolische KI noch eine Rolle? Oder ist symbolische KI gerade im Zusammenspiel mit Machine Learning sehr mächtig (siehe Reasoning Modelle bei den LLMs)?

3. Wie kompliziert sich die Wumpus-Welt, wenn der Wumpus selbst auch ein Agent wird und die Umwelt wahrnehmen kann sowie Aktionen auswählen kann?
